{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "repo_dir = os.getcwd()\n",
    "# weights_path = 'weights/yolov5-7.0/COCO_yolov5s6.pt'\n",
    "# weights_path = 'weights/yolov5-6.2/Sample_yolov5s6_300_epochs.pt'\n",
    "weights_path = 'weights/yolov5-3.1/TACO_yolov5s_300_epochs.pt'\n",
    "# weights_path = 'weights/yolov5-3.1/Garbage_yolov5s_300_epochs.pt'\n",
    "model_load_path = os.path.join(repo_dir, weights_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  626dd31 Python-3.8.15 torch-1.12.1+cu113 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s-TACO summary: 232 layers, 7265397 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model = torch.hub.load(repo_dir, 'custom', path=model_load_path, source='local', device='cpu')\n",
    "# model = torch.hub.load(repo_dir, 'custom', path=yolov5s_model_path, source='local', device='cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Images\n",
    "# for f in 'zidane.jpg', 'bus.jpg':\n",
    "#     torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images\n",
    "# im1 = Image.open(os.path.join(repo_dir, 'data/images/zidane.jpg'))  # PIL image\n",
    "# im2 = cv2.imread(os.path.join(repo_dir, 'data/images/bus.jpg'))[..., ::-1]  # OpenCV image (BGR to RGB)\n",
    "im3 = Image.open(os.path.join(repo_dir, 'data/images/batch_1_000029.jpg'))  # PIL image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Inference\n",
    "# results = model(im1)  # batch of images\n",
    "# results = model([im1, im2], size=640)  # batch of images\n",
    "results = model([im3], size=640)  # batch of images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1: 2049x1537 1 Drink can\n",
      "Speed: 38.6ms pre-process, 192.3ms inference, 1.0ms NMS per image at shape (1, 3, 640, 512)\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "results.print()\n",
    "# results.save()\n",
    "results.show()\n",
    "# results.save()  # or .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "         xmin        ymin         xmax         ymax  confidence  class  \\\n0  654.954285  881.505066  1071.494507  1564.204102    0.915069      2   \n\n        name  \n0  Drink can  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>confidence</th>\n      <th>class</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>654.954285</td>\n      <td>881.505066</td>\n      <td>1071.494507</td>\n      <td>1564.204102</td>\n      <td>0.915069</td>\n      <td>2</td>\n      <td>Drink can</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.xyxy[0]  # im1 predictions (tensor)\n",
    "results.pandas().xyxy[0]  # im1 predictions (pandas)\n",
    "#      xmin    ymin    xmax   ymax  confidence  class    name\n",
    "# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n",
    "# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n",
    "# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n",
    "# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
